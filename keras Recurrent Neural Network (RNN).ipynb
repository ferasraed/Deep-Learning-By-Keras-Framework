{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6384d9bb-de56-429a-86a5-3dc5167ec7e7",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/drive/1dC6KP-6rDM_Hy6oUOqPWT2l9ywIz2qmK?usp=sharing\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc749089-4224-4e37-b59f-022bd87283b2",
   "metadata": {},
   "source": [
    "# ***Setup***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea1c4f95-1ce0-499a-b62b-813281fe095a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import text_dataset_from_directory\n",
    "from tensorflow.strings import regex_replace\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091c563-9e9d-4835-bf26-1943f313efa4",
   "metadata": {},
   "source": [
    "# ***Preparing the Data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9c3241c2-e32c-4387-b107-74c0c24748dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n",
      "b'Actually, they don\\'t, but they certainly did when trying to think of a singular line that adequately summarises how terrible this entry in the series really is. There were some moments that could have been good, but they are mostly outweighed by their own conversion into missed opportunities, and don\\'t get me started on the bad.  The wasted opportunities are pretty obvious, but I will recap them here in case anyone cares. Anyone who hasn\\'t seen the film and genuinely gives a toss would be advised to stop reading at this point. The first, and potentially the biggest, wasted opportunity, was the plot with Freddy\\'s long-lost child. Now, the extreme mental illness that Freddy appears to suffer (and I might hasten to add that less than one percent of mental patients are a threat to other people, leave alone to this extent) is HEREDITARY, so why not a mystery-type slasher in which Lisa Zane\\'s character dreams of Freddy murdering the teens, only we later discover it\\'s actually her doing all the killing? Sound like a good plot idea to you? Obviously it was above the heads of Talalay and De Luca.  Then there\\'s the trip to Springfield, where the entire adolescent population has been wiped out, and the remaining adults are experiencing a kind of mass psychosis. Funnily enough, said mass psychosis was actually depicted in a realistic and convincing manner, although this has a fair amount to do with the fact that we are never shown too much. We are just given quick visual hints of the massive loss of connection with reality that would stem from the grief of every youngster in town dying for reasons beyond one\\'s comprehension and control. The essential problem with this plot element, however, is that the town is abandoned too quickly, and with no real answers. This collection of scenes would have been far creepier with ten minutes of say... one sane citizen explaining to these visitors why the Springfield fair looks like a horror show.  Of course, horror films are never noted for their character development, unless they\\'re the kind of horror films John Carpenter used to direct, but how are we supposed to really care when characters we know next to nothing about die? At least Wes Craven took the time to set up his characters in the original, and used a few cheap tricks to draw the audience in. That, in a nutshell, is probably the biggest problem with Freddy\\'s Dead: it just doesn\\'t try at all, leave alone hard enough.  On a related note, I feel kind of sorry for Robert Englund, now that he is more or less inextricably linked with the Freddy character. He has played far better characters in far better productions (the science-fiction miniseries \"V\", for example), and to be forever remembered as \"the man who played Freddy\" is selling him rather short. It seems he will never break the mold of horror films now. As for the rest of the cast, well, I think their performances here speak for themselves. They deserved to be permanently typecast as little more than B-grade horror props. Even Yaphet Kotto doesn\\'t escape this one unscathed, as his character is one of the most childishly written in the history of B-films.  All in all, Freddy\\'s Dead gets a 1 out of me. I\\'d vote lower, but the IMDb doesn\\'t allow for that. FD is really a testament to how a writer\\'s inability to exploit a concept to the fullest extent can ruin not only a film, but an entire franchise.'\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "def prepareData(dir):\n",
    "  data = text_dataset_from_directory(dir)\n",
    "  return data.map(\n",
    "    lambda text, label: (regex_replace(text, '<br />', ' '), label),\n",
    ")\n",
    "\n",
    "train_data = prepareData(r\"C:\\Users\\feras\\Desktop\\All\\Research Notebooks\\movie-reviews-dataset\\train\")\n",
    "test_data = prepareData(r\"C:\\Users\\feras\\Desktop\\All\\Research Notebooks\\movie-reviews-dataset\\test\")\n",
    "\n",
    "for text_batch, label_batch in train_data.take(1):\n",
    "  print(text_batch.numpy()[0])\n",
    "  print(label_batch.numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c8d98f-bc80-469f-940c-596e21028c0e",
   "metadata": {},
   "source": [
    "# ***Building the Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc9a8c37-c233-4567-82e2-de72267bb213",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(1,), dtype=\"string\"))\n",
    "\n",
    "# Text Vectorization\n",
    "max_tokens = 1000\n",
    "max_len = 100\n",
    "vectorize_layer = TextVectorization(\n",
    "  max_tokens=max_tokens,\n",
    "  output_mode=\"int\",\n",
    "  output_sequence_length=max_len,\n",
    ")\n",
    "\n",
    "train_texts = train_data.map(lambda text, label: text)\n",
    "vectorize_layer.adapt(train_texts)\n",
    "\n",
    "model.add(vectorize_layer)\n",
    "\n",
    "# Embedding\n",
    "model.add(Embedding(max_tokens + 1, 128))\n",
    "\n",
    "# Recurrent Layer (Long Short-Term Memory (LSTM))\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(64, dropout=0.25, recurrent_dropout=0.25))\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a17f8f-6e07-46d8-8652-3bbeb2825f2b",
   "metadata": {},
   "source": [
    "# ***Compiling the Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9848f0a7-9ba2-4136-ab4c-8e023c42bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "  optimizer=Adam(learning_rate=0.005),\n",
    "  loss='binary_crossentropy',\n",
    "  metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38859cb-8d47-4ec8-8b11-bd84477c183a",
   "metadata": {},
   "source": [
    "# ***Training the Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d78dcfb8-604e-4d86-a70e-2d1d6c5b257f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 80ms/step - accuracy: 0.5667 - loss: 0.6757\n",
      "Epoch 2/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 75ms/step - accuracy: 0.7349 - loss: 0.5456\n",
      "Epoch 3/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 76ms/step - accuracy: 0.7766 - loss: 0.4807\n",
      "Epoch 4/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 73ms/step - accuracy: 0.7981 - loss: 0.4472\n",
      "Epoch 5/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 85ms/step - accuracy: 0.8074 - loss: 0.4257\n",
      "Epoch 6/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 66ms/step - accuracy: 0.8175 - loss: 0.4122\n",
      "Epoch 7/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 65ms/step - accuracy: 0.8225 - loss: 0.4003\n",
      "Epoch 8/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 65ms/step - accuracy: 0.8262 - loss: 0.3906\n",
      "Epoch 9/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 65ms/step - accuracy: 0.8256 - loss: 0.3845\n",
      "Epoch 10/10\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 65ms/step - accuracy: 0.8371 - loss: 0.3682\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1ca21e12250>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_data, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821d0aa8-6a76-41b0-b22c-2d4e5c42018a",
   "metadata": {},
   "source": [
    "# ***Using the Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef88d272-0699-4dab-b63d-efee74b862bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('rnn.weights.h5')\n",
    "model.load_weights('rnn.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474fa2b7-8f3c-4b8d-9ea9-a258043e00e9",
   "metadata": {},
   "source": [
    "# ***Testing the Model***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eefd129d-a791-4803-aecd-9d5391c15ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 57ms/step - accuracy: 0.8010 - loss: 0.4225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4301472306251526, 0.7976400256156921]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
